{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGRU.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-BExb9A_gIY"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import datasets\n",
        "from torchtext import data\n",
        "from torchtext.data import Field, LabelField\n",
        "from torchtext.data import BucketIterator\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90BTfOL_HDQ5"
      },
      "source": [
        "#Prepare model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY5CLLvZAIon"
      },
      "source": [
        "class GRU_Text(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 emb_dim,\n",
        "                 hidden_dim,\n",
        "                 out_dim,\n",
        "                 dropout,\n",
        "                 pad_idx\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size,\n",
        "            emb_dim,\n",
        "            padding_idx=pad_idx\n",
        "            )\n",
        "        self.rnn = nn.GRU(\n",
        "            emb_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=1,\n",
        "            bidirectional=True\n",
        "            )\n",
        "        self.fc = nn.Linear(hidden_dim * 2, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        #text = [sent len, batch size]\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.detach().cpu())\n",
        "        packed_output, hidden = self.rnn(packed_embedded)\n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        #output = [sent len, batch size, hid dim * num directions]\n",
        "        #output over padding tokens are zero tensors\n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpzVWhc7HE6y"
      },
      "source": [
        "# Prepare dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kdw3Hi5HCXE"
      },
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "train_data, test_data = datasets.TREC.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split()\n",
        "TEXT.build_vocab(train_data, max_size=25000, vectors='glove.6B.100d', unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCXg1obVKWLP"
      },
      "source": [
        "Add pretrained embeddings to increase quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPlHru5_KbRS"
      },
      "source": [
        "model = GRU_Text(\n",
        "    vocab_size=len(TEXT.vocab),\n",
        "    emb_dim=100,\n",
        "    hidden_dim=128,\n",
        "    out_dim=6,\n",
        "    dropout=0.1,\n",
        "    pad_idx=TEXT.vocab.stoi[TEXT.pad_token]\n",
        "    )"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJw1RceCHlX_",
        "outputId": "aea8f2df-c6b6-4211-ea5b-2161aea8afd0"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5494, -1.0460,  0.4548,  ...,  1.5497,  0.6393, -1.4256],\n",
              "        [-0.0999,  0.1005, -1.6691,  ...,  1.3173, -0.3409, -0.1412],\n",
              "        [ 0.1638,  0.6046,  1.0789,  ..., -0.3140,  0.1844,  0.3624],\n",
              "        ...,\n",
              "        [ 0.0091,  0.2810,  0.7356,  ..., -0.7508,  0.8967, -0.7631],\n",
              "        [ 0.2906,  0.3217,  0.2419,  ..., -0.9444, -0.3790,  0.6196],\n",
              "        [-0.2803,  0.3769, -1.3597,  ..., -0.5477,  0.3796,  1.9748]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAL_6AGAKaLt"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(100)\n",
        "model.embedding.weight.data[TEXT.vocab.stoi[TEXT.pad_token]] = torch.zeros(100)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcuB3T9KLthY"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDsfpAEDL2AH"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAlRpJk8L-BM"
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim=1, keepdim=True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    correct = correct.detach().to('cpu')\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5nAT8OsMFgi"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text, text_lengths = batch.text\n",
        "        predictions = model(text, text_lengths)\n",
        "        loss = criterion(predictions, batch.label.long())\n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1NcO2waMMnJ"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, text_lengths = batch.text\n",
        "            predictions = model(text, text_lengths)\n",
        "            loss = criterion(predictions, batch.label.long())\n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7jCjQKeMRAm"
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpN418NeMTHN",
        "outputId": "d88491f4-e333-45a4-bd9e-67f52fca1f08"
      },
      "source": [
        "N_EPOCHS = 50\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'textrnn_trec.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.251 | Train Acc: 50.20%\n",
            "\t Val. Loss: 0.966 |  Val. Acc: 63.03%\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.792 | Train Acc: 71.71%\n",
            "\t Val. Loss: 0.689 |  Val. Acc: 74.79%\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.578 | Train Acc: 79.70%\n",
            "\t Val. Loss: 0.560 |  Val. Acc: 78.65%\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.420 | Train Acc: 84.92%\n",
            "\t Val. Loss: 0.491 |  Val. Acc: 82.05%\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.311 | Train Acc: 89.81%\n",
            "\t Val. Loss: 0.456 |  Val. Acc: 83.73%\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train Acc: 93.27%\n",
            "\t Val. Loss: 0.423 |  Val. Acc: 84.95%\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.156 | Train Acc: 95.16%\n",
            "\t Val. Loss: 0.427 |  Val. Acc: 84.83%\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.101 | Train Acc: 97.11%\n",
            "\t Val. Loss: 0.438 |  Val. Acc: 85.38%\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train Acc: 98.10%\n",
            "\t Val. Loss: 0.487 |  Val. Acc: 85.60%\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.051 | Train Acc: 98.54%\n",
            "\t Val. Loss: 0.445 |  Val. Acc: 86.20%\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.033 | Train Acc: 99.38%\n",
            "\t Val. Loss: 0.458 |  Val. Acc: 86.02%\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.026 | Train Acc: 99.27%\n",
            "\t Val. Loss: 0.493 |  Val. Acc: 86.55%\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.017 | Train Acc: 99.71%\n",
            "\t Val. Loss: 0.548 |  Val. Acc: 86.85%\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.013 | Train Acc: 99.78%\n",
            "\t Val. Loss: 0.542 |  Val. Acc: 87.21%\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.007 | Train Acc: 99.95%\n",
            "\t Val. Loss: 0.569 |  Val. Acc: 87.51%\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.012 | Train Acc: 99.75%\n",
            "\t Val. Loss: 0.588 |  Val. Acc: 85.90%\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.017 | Train Acc: 99.56%\n",
            "\t Val. Loss: 0.530 |  Val. Acc: 86.85%\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.009 | Train Acc: 99.82%\n",
            "\t Val. Loss: 0.533 |  Val. Acc: 86.68%\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.006 | Train Acc: 99.92%\n",
            "\t Val. Loss: 0.687 |  Val. Acc: 86.32%\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.008 | Train Acc: 99.82%\n",
            "\t Val. Loss: 0.573 |  Val. Acc: 86.98%\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.005 | Train Acc: 99.90%\n",
            "\t Val. Loss: 0.643 |  Val. Acc: 86.56%\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.005 | Train Acc: 99.84%\n",
            "\t Val. Loss: 0.632 |  Val. Acc: 86.20%\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.97%\n",
            "\t Val. Loss: 0.586 |  Val. Acc: 86.68%\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.92%\n",
            "\t Val. Loss: 0.724 |  Val. Acc: 85.37%\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.004 | Train Acc: 99.95%\n",
            "\t Val. Loss: 0.639 |  Val. Acc: 86.08%\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.002 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.635 |  Val. Acc: 86.68%\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.002 | Train Acc: 99.97%\n",
            "\t Val. Loss: 0.599 |  Val. Acc: 87.34%\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.006 | Train Acc: 99.74%\n",
            "\t Val. Loss: 0.610 |  Val. Acc: 86.02%\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.018 | Train Acc: 99.48%\n",
            "\t Val. Loss: 0.571 |  Val. Acc: 86.42%\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.005 | Train Acc: 99.95%\n",
            "\t Val. Loss: 0.597 |  Val. Acc: 86.69%\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.002 | Train Acc: 99.95%\n",
            "\t Val. Loss: 0.665 |  Val. Acc: 86.56%\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.92%\n",
            "\t Val. Loss: 0.648 |  Val. Acc: 87.04%\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.95%\n",
            "\t Val. Loss: 0.633 |  Val. Acc: 86.80%\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.605 |  Val. Acc: 87.04%\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.645 |  Val. Acc: 87.16%\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.635 |  Val. Acc: 87.22%\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.639 |  Val. Acc: 87.22%\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.001 | Train Acc: 99.97%\n",
            "\t Val. Loss: 0.709 |  Val. Acc: 86.56%\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.707 |  Val. Acc: 86.56%\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.663 |  Val. Acc: 86.92%\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.645 |  Val. Acc: 87.40%\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.673 |  Val. Acc: 87.04%\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.666 |  Val. Acc: 87.70%\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.730 |  Val. Acc: 86.85%\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.678 |  Val. Acc: 86.86%\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.730 |  Val. Acc: 86.33%\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.002 | Train Acc: 99.95%\n",
            "\t Val. Loss: 0.700 |  Val. Acc: 86.81%\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.001 | Train Acc: 99.97%\n",
            "\t Val. Loss: 0.738 |  Val. Acc: 85.97%\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
            "\t Val. Loss: 0.737 |  Val. Acc: 86.75%\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.001 | Train Acc: 99.97%\n",
            "\t Val. Loss: 0.807 |  Val. Acc: 85.80%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEkXD3l8Mb0p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HwEGtHKG9KQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}