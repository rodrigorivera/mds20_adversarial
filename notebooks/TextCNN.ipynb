{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextCNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTVAB5YkRJHg"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from IPython.display import clear_output\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rc481yfdMMj"
      },
      "source": [
        "#Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_3a6SxFdiXC",
        "outputId": "d2109010-ff6a-4605-90b1-d2d7b478bae9"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_dataset = load_dataset('trec', split='train')\n",
        "train_dataset = train_dataset.map(\n",
        "    lambda e: tokenizer(e['text'], truncation=True, padding='max_length'),\n",
        "    batched=True\n",
        "    )\n",
        "test_dataset = load_dataset('trec', split='test')\n",
        "test_dataset = test_dataset.map(\n",
        "    lambda e: tokenizer(e['text'], truncation=True, padding='max_length'),\n",
        "    batched=True\n",
        "    )\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=64)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset trec (/root/.cache/huggingface/datasets/trec/default/1.1.0/1902c380fe66cc215f989888b1b35e8da7e79a3a97520f00dce753fd1f8f5c48)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/trec/default/1.1.0/1902c380fe66cc215f989888b1b35e8da7e79a3a97520f00dce753fd1f8f5c48/cache-a128e73c7344c66a.arrow\n",
            "Using custom data configuration default\n",
            "Reusing dataset trec (/root/.cache/huggingface/datasets/trec/default/1.1.0/1902c380fe66cc215f989888b1b35e8da7e79a3a97520f00dce753fd1f8f5c48)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/trec/default/1.1.0/1902c380fe66cc215f989888b1b35e8da7e79a3a97520f00dce753fd1f8f5c48/cache-628b05de24021c11.arrow\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l4tSBxLd9Ya"
      },
      "source": [
        "# Build model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udwzyH_LOz2B"
      },
      "source": [
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels=embedding_dim, out_channels=n_filters, kernel_size=fs) for fs in filter_sizes])\n",
        "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        embs = self.embedding(text)\n",
        "        embs = embs.permute(0, 2, 1)\n",
        "        out = [F.relu(c(embs)) for c in self.convs]\n",
        "        out_pool = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in out]\n",
        "        cat = self.dropout(torch.cat(out_pool, dim=1))\n",
        "        final = self.fc(cat)\n",
        "        return final"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmMH9AAoGA45"
      },
      "source": [
        "#set device\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12lsQYtiRCE_"
      },
      "source": [
        "model = TextCNN(vocab_size=tokenizer.vocab_size,\n",
        "                embedding_dim=100,\n",
        "                n_filters=8,\n",
        "                filter_sizes=[3,4,5],\n",
        "                output_dim=6,\n",
        "                dropout=0.1,\n",
        "                pad_idx=tokenizer.pad_token_id)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zBb0vYEpgwn"
      },
      "source": [
        "def trec_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim=1, keepdim=True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    correct = correct.detach().to('cpu')\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBusF2HBiBGx"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, tokenizer):\n",
        "    global device\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    half = len(iterator) // 2 \n",
        "    for i, batch in enumerate(iterator):\n",
        "        if i <= half:\n",
        "            batch_ = torch.stack(batch['input_ids'], dim=0).permute(1, 0)\n",
        "            batch_ = batch_.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(batch_)\n",
        "            loss = criterion(predictions, batch['label-coarse'].long().to(device))\n",
        "            acc = trec_accuracy(predictions, batch['label-coarse'].to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        else:\n",
        "            break\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGusoctzpMe-"
      },
      "source": [
        "def evaluate(model, iterator, criterion, tokenizer):\n",
        "    global device\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            batch_ = torch.stack(batch['input_ids'], dim=0).permute(1, 0)\n",
        "            batch_ = batch_.to(device)\n",
        "            predictions = model(batch_)\n",
        "            loss = criterion(predictions, batch['label-coarse'].long().to(device))\n",
        "            acc = trec_accuracy(predictions, batch['label-coarse'].to(device))\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raWL_TaBqve2"
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hgJTbkwqzJd",
        "outputId": "88760300-d75d-457b-8704-9f28034cd64d"
      },
      "source": [
        "N_EPOCHS = 50\n",
        "best_valid_loss = float('inf')\n",
        "losses = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(model, trainloader, optimizer, criterion, tokenizer)\n",
        "    valid_loss, valid_acc = evaluate(model, testloader, criterion, tokenizer)\n",
        "    losses.append(train_loss)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'textcnn_trec.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "    if epoch >= 3:\n",
        "        if train_loss >= losses[-1] and train_loss >= losses[-2] and train_loss >= losses[-3]:\n",
        "            print('Early stopping')\n",
        "            break "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.807 | Train Acc: 16.84%\n",
            "\t Val. Loss: 1.337 |  Val. Acc: 53.17%\n",
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.606 | Train Acc: 30.12%\n",
            "\t Val. Loss: 1.031 |  Val. Acc: 68.24%\n",
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.480 | Train Acc: 35.30%\n",
            "\t Val. Loss: 0.854 |  Val. Acc: 72.13%\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.400 | Train Acc: 37.74%\n",
            "\t Val. Loss: 0.756 |  Val. Acc: 76.13%\n",
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.347 | Train Acc: 39.34%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 77.10%\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.301 | Train Acc: 41.37%\n",
            "\t Val. Loss: 0.657 |  Val. Acc: 77.00%\n",
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.262 | Train Acc: 42.82%\n",
            "\t Val. Loss: 0.620 |  Val. Acc: 78.71%\n",
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.228 | Train Acc: 44.20%\n",
            "\t Val. Loss: 0.597 |  Val. Acc: 78.56%\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.187 | Train Acc: 45.97%\n",
            "\t Val. Loss: 0.581 |  Val. Acc: 78.47%\n",
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.165 | Train Acc: 46.60%\n",
            "\t Val. Loss: 0.567 |  Val. Acc: 80.08%\n",
            "Epoch: 11 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.149 | Train Acc: 47.02%\n",
            "\t Val. Loss: 0.562 |  Val. Acc: 79.64%\n",
            "Epoch: 12 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.122 | Train Acc: 48.13%\n",
            "\t Val. Loss: 0.548 |  Val. Acc: 80.08%\n",
            "Epoch: 13 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.110 | Train Acc: 48.53%\n",
            "\t Val. Loss: 0.544 |  Val. Acc: 81.01%\n",
            "Epoch: 14 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.094 | Train Acc: 48.93%\n",
            "\t Val. Loss: 0.547 |  Val. Acc: 79.94%\n",
            "Epoch: 15 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.080 | Train Acc: 49.35%\n",
            "\t Val. Loss: 0.552 |  Val. Acc: 80.18%\n",
            "Epoch: 16 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.071 | Train Acc: 49.60%\n",
            "\t Val. Loss: 0.545 |  Val. Acc: 79.01%\n",
            "Epoch: 17 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.064 | Train Acc: 49.73%\n",
            "\t Val. Loss: 0.540 |  Val. Acc: 80.33%\n",
            "Epoch: 18 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.058 | Train Acc: 49.71%\n",
            "\t Val. Loss: 0.539 |  Val. Acc: 79.94%\n",
            "Epoch: 19 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.054 | Train Acc: 49.84%\n",
            "\t Val. Loss: 0.543 |  Val. Acc: 79.70%\n",
            "Epoch: 20 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.046 | Train Acc: 50.09%\n",
            "\t Val. Loss: 0.547 |  Val. Acc: 79.94%\n",
            "Epoch: 21 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.041 | Train Acc: 50.22%\n",
            "\t Val. Loss: 0.541 |  Val. Acc: 80.14%\n",
            "Epoch: 22 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.038 | Train Acc: 50.44%\n",
            "\t Val. Loss: 0.546 |  Val. Acc: 79.55%\n",
            "Epoch: 23 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.036 | Train Acc: 50.36%\n",
            "\t Val. Loss: 0.551 |  Val. Acc: 80.53%\n",
            "Epoch: 24 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.033 | Train Acc: 50.42%\n",
            "\t Val. Loss: 0.551 |  Val. Acc: 80.14%\n",
            "Epoch: 25 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.032 | Train Acc: 50.42%\n",
            "\t Val. Loss: 0.557 |  Val. Acc: 79.79%\n",
            "Epoch: 26 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.026 | Train Acc: 50.65%\n",
            "\t Val. Loss: 0.557 |  Val. Acc: 79.79%\n",
            "Epoch: 27 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.028 | Train Acc: 50.55%\n",
            "\t Val. Loss: 0.556 |  Val. Acc: 80.33%\n",
            "Epoch: 28 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.023 | Train Acc: 50.65%\n",
            "\t Val. Loss: 0.556 |  Val. Acc: 81.20%\n",
            "Epoch: 29 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.020 | Train Acc: 50.82%\n",
            "\t Val. Loss: 0.559 |  Val. Acc: 79.94%\n",
            "Epoch: 30 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.020 | Train Acc: 50.76%\n",
            "\t Val. Loss: 0.558 |  Val. Acc: 80.66%\n",
            "Epoch: 31 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.021 | Train Acc: 50.65%\n",
            "\t Val. Loss: 0.559 |  Val. Acc: 80.81%\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L85rTHw-XSw3"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}