{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextCNN_trec_dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTVAB5YkRJHg"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext import datasets\n",
        "from torchtext import data\n",
        "from torchtext.data import Field, LabelField\n",
        "from torchtext.data import BucketIterator\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPXJPUOOQ0kh"
      },
      "source": [
        "The substitute classifier is a Convolutional Neural Network (CNN) for Sentence Classification proposed in [18]. In our experiments, the CNN consists of multiple convolution layers and max pooling layers. The CNN has one convolution layer for each of the n-gram filter sizes. Each convolution operation gives out a vector of size num_filters. We use [3, 5] n-gram filter sizes with num_filters = 8. The size of the learned embedding matrix E remains the same e = 100. The dropout ratio during training is set to 0.1. For this classifier the accuracy values provided in the main paper are also high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rc481yfdMMj"
      },
      "source": [
        "#Build vocab "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_3a6SxFdiXC",
        "outputId": "1923252d-5460-4fd3-e6d3-1d6cc5bc05f8"
      },
      "source": [
        "TEXT = data.Field(tokenize='spacy', batch_first=True)\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "train_data, test_data = datasets.TREC.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split()\n",
        "MAX_VOCAB_SIZE = 100000\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading train_5500.label\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train_5500.label:   0%|          | 0.00/336k [00:00<?, ?B/s]\u001b[A\n",
            "train_5500.label:  20%|█▉        | 65.5k/336k [00:00<00:00, 517kB/s]\u001b[A\n",
            "train_5500.label: 100%|██████████| 336k/336k [00:00<00:00, 1.27MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading TREC_10.label\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TREC_10.label: 100%|██████████| 23.4k/23.4k [00:00<00:00, 393kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l4tSBxLd9Ya"
      },
      "source": [
        "# Build model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udwzyH_LOz2B"
      },
      "source": [
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels=embedding_dim, out_channels=n_filters, kernel_size=fs) for fs in filter_sizes])\n",
        "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [batch size, sent len]\n",
        "        embs = self.embedding(text)\n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        embs = embs.permute(0, 2, 1)\n",
        "        #embedded = [batch size, emb dim, sent len]\n",
        "        out = [F.relu(c(embs)) for c in self.convs]\n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "        out_pool = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in out]\n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        cat = self.dropout(torch.cat(out_pool, dim=1))\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "        final = self.fc(cat)\n",
        "        return final"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12lsQYtiRCE_"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "model = TextCNN(vocab_size=len(TEXT.vocab),\n",
        "                embedding_dim=100,\n",
        "                n_filters=8,\n",
        "                filter_sizes=[3,4,5],\n",
        "                output_dim=6,\n",
        "                dropout=0.1,\n",
        "                pad_idx=1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zBb0vYEpgwn"
      },
      "source": [
        "def trec_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    correct = []\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.softmax(preds, dim=1))\n",
        "    for x, yy in zip(rounded_preds, y): \n",
        "        correct.append((torch.argmax(x) == yy).float()) #convert into float for division\n",
        "    acc = np.sum(correct) / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBusF2HBiBGx"
      },
      "source": [
        "def train_model(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label.type(torch.LongTensor).to(device))\n",
        "        acc = trec_accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGusoctzpMe-"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label.type(torch.LongTensor).to(device))\n",
        "            acc = trec_accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raWL_TaBqve2"
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idF_OX9ird97",
        "outputId": "fa7e78f9-4998-405f-d597-5e6ebb6b6d4a"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7840,  0.5547, -1.9164,  ...,  1.0417,  0.9624, -0.7122],\n",
              "        [ 0.0989, -0.5916,  0.7481,  ...,  0.4283, -1.3365, -0.4723],\n",
              "        [ 0.1638,  0.6046,  1.0789,  ..., -0.3140,  0.1844,  0.3624],\n",
              "        ...,\n",
              "        [ 1.1097,  0.3746, -0.3882,  ..., -0.4966,  0.2572, -0.9995],\n",
              "        [ 0.0091,  0.2810,  0.7356,  ..., -0.7508,  0.8967, -0.7631],\n",
              "        [ 0.2269, -0.7989, -2.0088,  ...,  1.0667,  0.5483,  1.4923]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hgJTbkwqzJd",
        "outputId": "b5e5a297-bf68-4998-87b7-5940f012270c"
      },
      "source": [
        "N_EPOCHS = 50\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_model(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'textcnn_trec.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.045 | Train Acc: 50.70%\n",
            "\t Val. Loss: 0.949 |  Val. Acc: 53.89%\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.814 | Train Acc: 63.83%\n",
            "\t Val. Loss: 0.796 |  Val. Acc: 66.11%\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.647 | Train Acc: 73.30%\n",
            "\t Val. Loss: 0.707 |  Val. Acc: 70.16%\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.519 | Train Acc: 79.08%\n",
            "\t Val. Loss: 0.646 |  Val. Acc: 74.04%\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.423 | Train Acc: 83.20%\n",
            "\t Val. Loss: 0.612 |  Val. Acc: 76.38%\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.334 | Train Acc: 88.04%\n",
            "\t Val. Loss: 0.578 |  Val. Acc: 77.70%\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.280 | Train Acc: 90.17%\n",
            "\t Val. Loss: 0.579 |  Val. Acc: 78.12%\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train Acc: 92.92%\n",
            "\t Val. Loss: 0.561 |  Val. Acc: 78.37%\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.178 | Train Acc: 94.37%\n",
            "\t Val. Loss: 0.580 |  Val. Acc: 78.85%\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.146 | Train Acc: 95.75%\n",
            "\t Val. Loss: 0.580 |  Val. Acc: 78.00%\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.120 | Train Acc: 96.47%\n",
            "\t Val. Loss: 0.575 |  Val. Acc: 79.49%\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.107 | Train Acc: 97.13%\n",
            "\t Val. Loss: 0.592 |  Val. Acc: 78.77%\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train Acc: 97.82%\n",
            "\t Val. Loss: 0.635 |  Val. Acc: 78.55%\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train Acc: 97.99%\n",
            "\t Val. Loss: 0.610 |  Val. Acc: 79.27%\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train Acc: 98.36%\n",
            "\t Val. Loss: 0.634 |  Val. Acc: 78.74%\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.051 | Train Acc: 98.75%\n",
            "\t Val. Loss: 0.618 |  Val. Acc: 79.33%\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.049 | Train Acc: 98.89%\n",
            "\t Val. Loss: 0.636 |  Val. Acc: 79.63%\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.041 | Train Acc: 99.17%\n",
            "\t Val. Loss: 0.647 |  Val. Acc: 79.57%\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.038 | Train Acc: 99.17%\n",
            "\t Val. Loss: 0.642 |  Val. Acc: 79.43%\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.037 | Train Acc: 99.18%\n",
            "\t Val. Loss: 0.664 |  Val. Acc: 79.73%\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.033 | Train Acc: 99.40%\n",
            "\t Val. Loss: 0.671 |  Val. Acc: 79.73%\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.025 | Train Acc: 99.69%\n",
            "\t Val. Loss: 0.701 |  Val. Acc: 79.55%\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.024 | Train Acc: 99.56%\n",
            "\t Val. Loss: 0.698 |  Val. Acc: 79.69%\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.020 | Train Acc: 99.74%\n",
            "\t Val. Loss: 0.717 |  Val. Acc: 79.39%\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.021 | Train Acc: 99.58%\n",
            "\t Val. Loss: 0.725 |  Val. Acc: 79.09%\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.019 | Train Acc: 99.59%\n",
            "\t Val. Loss: 0.716 |  Val. Acc: 80.03%\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.021 | Train Acc: 99.45%\n",
            "\t Val. Loss: 0.729 |  Val. Acc: 79.67%\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.016 | Train Acc: 99.71%\n",
            "\t Val. Loss: 0.719 |  Val. Acc: 80.46%\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.020 | Train Acc: 99.43%\n",
            "\t Val. Loss: 0.749 |  Val. Acc: 79.81%\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.015 | Train Acc: 99.58%\n",
            "\t Val. Loss: 0.758 |  Val. Acc: 79.87%\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.014 | Train Acc: 99.69%\n",
            "\t Val. Loss: 0.781 |  Val. Acc: 79.03%\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.012 | Train Acc: 99.90%\n",
            "\t Val. Loss: 0.780 |  Val. Acc: 79.73%\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.015 | Train Acc: 99.61%\n",
            "\t Val. Loss: 0.757 |  Val. Acc: 80.34%\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.013 | Train Acc: 99.74%\n",
            "\t Val. Loss: 0.786 |  Val. Acc: 79.67%\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.011 | Train Acc: 99.84%\n",
            "\t Val. Loss: 0.804 |  Val. Acc: 79.37%\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.013 | Train Acc: 99.75%\n",
            "\t Val. Loss: 0.820 |  Val. Acc: 79.19%\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.010 | Train Acc: 99.77%\n",
            "\t Val. Loss: 0.828 |  Val. Acc: 78.79%\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.010 | Train Acc: 99.79%\n",
            "\t Val. Loss: 0.818 |  Val. Acc: 80.15%\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.010 | Train Acc: 99.87%\n",
            "\t Val. Loss: 0.838 |  Val. Acc: 79.55%\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.008 | Train Acc: 99.87%\n",
            "\t Val. Loss: 0.793 |  Val. Acc: 80.70%\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.010 | Train Acc: 99.77%\n",
            "\t Val. Loss: 0.835 |  Val. Acc: 80.22%\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.008 | Train Acc: 99.79%\n",
            "\t Val. Loss: 0.845 |  Val. Acc: 80.22%\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.007 | Train Acc: 99.84%\n",
            "\t Val. Loss: 0.851 |  Val. Acc: 80.46%\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.008 | Train Acc: 99.87%\n",
            "\t Val. Loss: 0.845 |  Val. Acc: 80.46%\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.007 | Train Acc: 99.87%\n",
            "\t Val. Loss: 0.901 |  Val. Acc: 79.67%\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.009 | Train Acc: 99.74%\n",
            "\t Val. Loss: 0.870 |  Val. Acc: 80.03%\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.007 | Train Acc: 99.82%\n",
            "\t Val. Loss: 0.900 |  Val. Acc: 79.27%\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.005 | Train Acc: 99.90%\n",
            "\t Val. Loss: 0.908 |  Val. Acc: 79.49%\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.009 | Train Acc: 99.77%\n",
            "\t Val. Loss: 0.889 |  Val. Acc: 79.33%\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.006 | Train Acc: 99.92%\n",
            "\t Val. Loss: 0.873 |  Val. Acc: 79.97%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omD7kOdJydK1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
