{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLM+GRU+baseline_attack.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PHKUss5DTMif"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvZ9QMH2INS7",
        "outputId": "c9d1362a-8bc5-4325-e385-a5a9d4550006"
      },
      "source": [
        "!pip3 install torchtext==0.4.0\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.4.0 in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.4.0) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.4.0) (0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLuQq-jFjVId"
      },
      "source": [
        "# Train MLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5sE-SyGOC0H"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=25000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class MaskedLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYvzSxPvM3tD"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32osJtwbJgAR"
      },
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "TEXT = torchtext.data.Field(tokenize='spacy', init_token='<sos>', eos_token='<eos>', lower=True)\n",
        "LABEL = torchtext.data.LabelField(dtype = torch.float)\n",
        "train_data, test_data = torchtext.datasets.TREC.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split()\n",
        "TEXT.build_vocab(train_data, max_size=25000)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = 64,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ5VMi1JP2_g"
      },
      "source": [
        "bptt = 64\n",
        "# def get_batch(source, i):\n",
        "#     seq_len = min(bptt, len(source) - 1 - i)\n",
        "#     data = source[i:i+seq_len]\n",
        "#     target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "#     return data, target"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I--4P4TeS5sA"
      },
      "source": [
        "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
        "emsize = 64 # embedding dimension\n",
        "nhid = 64 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 4 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 4 # the number of heads in the multiheadattention models\n",
        "dropout = 0.1 # the dropout value\n",
        "model = MaskedLanguageModel(ntokens, emsize, nhead, nhid, nlayers, dropout)\n",
        "model = model.to(device)"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSCDFn_JTMh8"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    for batch, i in zip(train_iterator, range(len(train_iterator))):\n",
        "        data = batch.text\n",
        "        # data, targets = get_batch(batch.text, i)\n",
        "        optimizer.zero_grad()\n",
        "        if data.size(0) != bptt:\n",
        "            src_mask = model.generate_square_subsequent_mask(data.size(0))\n",
        "            src_mask = src_mask.to(device)\n",
        "        output = model(data, src_mask)\n",
        "        # print('output.view(-1, ntokens):', output.shape)\n",
        "        # print('data.reshape(-1)', data.reshape(-1).shape)\n",
        "        loss = criterion(output.view(data.reshape(-1).shape[0], -1), data.reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 10\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "            # cur_loss = total_loss / len(train_iterator)\n",
        "            cur_loss = loss.item()\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  ' ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, i, len(train_data) // bptt,\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_iterator):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for batch, i in zip(data_iterator, range(len(data_iterator))):\n",
        "            data = batch.text\n",
        "            if data.size(0) != bptt:\n",
        "                src_mask = model.generate_square_subsequent_mask(data.size(0))\n",
        "                src_mask = src_mask.to(device)\n",
        "            output = eval_model(data, src_mask)\n",
        "            # print(output.shape)\n",
        "            # print(data.shape)\n",
        "            # output_flat = output.view(-1, ntokens)\n",
        "            loss_ = len(data) * criterion(output.view(data.reshape(-1).shape[0], -1), data.reshape(-1)).item()\n",
        "            total_loss += loss_\n",
        "    return total_loss / len(data_iterator)"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP_rL43kTYeo",
        "outputId": "6762e98c-9bd5-47fb-e31d-09d2cde46990"
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "losses = []\n",
        "epochs = 50 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, valid_iterator)\n",
        "    losses.append(val_loss)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "    elif epoch >=3:\n",
        "        if val_loss >= losses[-1] and val_loss >= losses[-2]:\n",
        "            print('early stopping')\n",
        "            torch.save(best_model.state_dict(), 'MLM_wikitext.pt')\n",
        "            break "
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |    10/   59 batches |  ms/batch 24.78 | loss  7.89 | ppl  2669.74\n",
            "| epoch   1 |    20/   59 batches |  ms/batch 18.98 | loss  7.07 | ppl  1180.75\n",
            "| epoch   1 |    30/   59 batches |  ms/batch 19.29 | loss  6.21 | ppl   498.43\n",
            "| epoch   1 |    40/   59 batches |  ms/batch 18.63 | loss  5.72 | ppl   305.84\n",
            "| epoch   1 |    50/   59 batches |  ms/batch 19.22 | loss  4.66 | ppl   105.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  1.32s | valid loss 62.04 | valid ppl 879311135729863882165977088.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    10/   59 batches |  ms/batch 21.17 | loss  4.97 | ppl   144.02\n",
            "| epoch   2 |    20/   59 batches |  ms/batch 18.66 | loss  2.82 | ppl    16.73\n",
            "| epoch   2 |    30/   59 batches |  ms/batch 19.05 | loss  3.10 | ppl    22.30\n",
            "| epoch   2 |    40/   59 batches |  ms/batch 18.88 | loss  3.29 | ppl    26.83\n",
            "| epoch   2 |    50/   59 batches |  ms/batch 19.87 | loss  2.91 | ppl    18.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  1.26s | valid loss 35.70 | valid ppl 3179690509002495.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    10/   59 batches |  ms/batch 21.02 | loss  2.76 | ppl    15.75\n",
            "| epoch   3 |    20/   59 batches |  ms/batch 18.00 | loss  2.02 | ppl     7.54\n",
            "| epoch   3 |    30/   59 batches |  ms/batch 18.48 | loss  2.24 | ppl     9.39\n",
            "| epoch   3 |    40/   59 batches |  ms/batch 17.84 | loss  1.87 | ppl     6.47\n",
            "| epoch   3 |    50/   59 batches |  ms/batch 17.94 | loss  2.01 | ppl     7.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  1.23s | valid loss 26.48 | valid ppl 315880286875.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    10/   59 batches |  ms/batch 20.58 | loss  2.11 | ppl     8.24\n",
            "| epoch   4 |    20/   59 batches |  ms/batch 18.36 | loss  1.96 | ppl     7.08\n",
            "| epoch   4 |    30/   59 batches |  ms/batch 17.56 | loss  1.49 | ppl     4.43\n",
            "| epoch   4 |    40/   59 batches |  ms/batch 17.76 | loss  1.62 | ppl     5.07\n",
            "| epoch   4 |    50/   59 batches |  ms/batch 18.05 | loss  1.22 | ppl     3.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time:  1.21s | valid loss 21.84 | valid ppl 3069230769.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    10/   59 batches |  ms/batch 20.92 | loss  1.42 | ppl     4.13\n",
            "| epoch   5 |    20/   59 batches |  ms/batch 17.58 | loss  1.48 | ppl     4.39\n",
            "| epoch   5 |    30/   59 batches |  ms/batch 17.96 | loss  1.12 | ppl     3.05\n",
            "| epoch   5 |    40/   59 batches |  ms/batch 17.50 | loss  1.27 | ppl     3.56\n",
            "| epoch   5 |    50/   59 batches |  ms/batch 18.98 | loss  1.11 | ppl     3.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time:  1.21s | valid loss 19.03 | valid ppl 184130397.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    10/   59 batches |  ms/batch 19.95 | loss  0.92 | ppl     2.51\n",
            "| epoch   6 |    20/   59 batches |  ms/batch 17.64 | loss  0.96 | ppl     2.62\n",
            "| epoch   6 |    30/   59 batches |  ms/batch 17.37 | loss  0.92 | ppl     2.52\n",
            "| epoch   6 |    40/   59 batches |  ms/batch 17.51 | loss  0.76 | ppl     2.13\n",
            "| epoch   6 |    50/   59 batches |  ms/batch 18.99 | loss  0.85 | ppl     2.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time:  1.19s | valid loss 16.78 | valid ppl 19445865.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    10/   59 batches |  ms/batch 20.01 | loss  0.83 | ppl     2.29\n",
            "| epoch   7 |    20/   59 batches |  ms/batch 17.96 | loss  0.60 | ppl     1.81\n",
            "| epoch   7 |    30/   59 batches |  ms/batch 16.91 | loss  0.73 | ppl     2.08\n",
            "| epoch   7 |    40/   59 batches |  ms/batch 18.39 | loss  0.78 | ppl     2.17\n",
            "| epoch   7 |    50/   59 batches |  ms/batch 17.62 | loss  0.83 | ppl     2.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time:  1.19s | valid loss 15.53 | valid ppl 5564532.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    10/   59 batches |  ms/batch 20.79 | loss  0.70 | ppl     2.01\n",
            "| epoch   8 |    20/   59 batches |  ms/batch 17.22 | loss  0.54 | ppl     1.71\n",
            "| epoch   8 |    30/   59 batches |  ms/batch 18.65 | loss  0.51 | ppl     1.67\n",
            "| epoch   8 |    40/   59 batches |  ms/batch 17.35 | loss  0.57 | ppl     1.76\n",
            "| epoch   8 |    50/   59 batches |  ms/batch 17.82 | loss  0.63 | ppl     1.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time:  1.20s | valid loss 14.45 | valid ppl 1889466.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    10/   59 batches |  ms/batch 19.30 | loss  0.43 | ppl     1.54\n",
            "| epoch   9 |    20/   59 batches |  ms/batch 18.50 | loss  0.32 | ppl     1.37\n",
            "| epoch   9 |    30/   59 batches |  ms/batch 17.20 | loss  0.56 | ppl     1.75\n",
            "| epoch   9 |    40/   59 batches |  ms/batch 17.59 | loss  0.44 | ppl     1.55\n",
            "| epoch   9 |    50/   59 batches |  ms/batch 17.06 | loss  0.45 | ppl     1.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time:  1.18s | valid loss 13.73 | valid ppl 915378.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    10/   59 batches |  ms/batch 20.80 | loss  0.43 | ppl     1.54\n",
            "| epoch  10 |    20/   59 batches |  ms/batch 17.29 | loss  0.32 | ppl     1.37\n",
            "| epoch  10 |    30/   59 batches |  ms/batch 17.23 | loss  0.36 | ppl     1.44\n",
            "| epoch  10 |    40/   59 batches |  ms/batch 17.78 | loss  0.46 | ppl     1.58\n",
            "| epoch  10 |    50/   59 batches |  ms/batch 17.36 | loss  0.35 | ppl     1.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time:  1.20s | valid loss 13.34 | valid ppl 621524.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |    10/   59 batches |  ms/batch 19.89 | loss  0.25 | ppl     1.28\n",
            "| epoch  11 |    20/   59 batches |  ms/batch 18.71 | loss  0.31 | ppl     1.37\n",
            "| epoch  11 |    30/   59 batches |  ms/batch 17.57 | loss  0.34 | ppl     1.40\n",
            "| epoch  11 |    40/   59 batches |  ms/batch 17.69 | loss  0.23 | ppl     1.26\n",
            "| epoch  11 |    50/   59 batches |  ms/batch 18.07 | loss  0.22 | ppl     1.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time:  1.20s | valid loss 13.04 | valid ppl 458926.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |    10/   59 batches |  ms/batch 19.78 | loss  0.21 | ppl     1.24\n",
            "| epoch  12 |    20/   59 batches |  ms/batch 17.81 | loss  0.20 | ppl     1.22\n",
            "| epoch  12 |    30/   59 batches |  ms/batch 17.07 | loss  0.18 | ppl     1.20\n",
            "| epoch  12 |    40/   59 batches |  ms/batch 17.87 | loss  0.17 | ppl     1.18\n",
            "| epoch  12 |    50/   59 batches |  ms/batch 17.94 | loss  0.26 | ppl     1.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time:  1.18s | valid loss 12.85 | valid ppl 381840.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |    10/   59 batches |  ms/batch 21.06 | loss  0.16 | ppl     1.17\n",
            "| epoch  13 |    20/   59 batches |  ms/batch 17.31 | loss  0.10 | ppl     1.11\n",
            "| epoch  13 |    30/   59 batches |  ms/batch 17.96 | loss  0.17 | ppl     1.19\n",
            "| epoch  13 |    40/   59 batches |  ms/batch 17.39 | loss  0.14 | ppl     1.15\n",
            "| epoch  13 |    50/   59 batches |  ms/batch 18.27 | loss  0.11 | ppl     1.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time:  1.19s | valid loss 12.78 | valid ppl 353475.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |    10/   59 batches |  ms/batch 20.29 | loss  0.10 | ppl     1.11\n",
            "| epoch  14 |    20/   59 batches |  ms/batch 17.33 | loss  0.09 | ppl     1.10\n",
            "| epoch  14 |    30/   59 batches |  ms/batch 17.99 | loss  0.12 | ppl     1.12\n",
            "| epoch  14 |    40/   59 batches |  ms/batch 17.26 | loss  0.08 | ppl     1.09\n",
            "| epoch  14 |    50/   59 batches |  ms/batch 18.62 | loss  0.08 | ppl     1.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time:  1.20s | valid loss 12.75 | valid ppl 344940.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |    10/   59 batches |  ms/batch 21.01 | loss  0.06 | ppl     1.06\n",
            "| epoch  15 |    20/   59 batches |  ms/batch 19.74 | loss  0.06 | ppl     1.07\n",
            "| epoch  15 |    30/   59 batches |  ms/batch 17.76 | loss  0.06 | ppl     1.07\n",
            "| epoch  15 |    40/   59 batches |  ms/batch 17.13 | loss  0.07 | ppl     1.08\n",
            "| epoch  15 |    50/   59 batches |  ms/batch 17.82 | loss  0.06 | ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time:  1.22s | valid loss 12.80 | valid ppl 363929.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoWgrssMU_yR",
        "outputId": "d562f208-dbc7-497b-a8a6-717cf4a84d6f"
      },
      "source": [
        "test_loss = evaluate(best_model, test_iterator)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  8.92 | test ppl  7490.68\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aGVAFHEI0DK"
      },
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GWXYo25bdod"
      },
      "source": [
        "# #load pretrained model\n",
        "# model.load_state_dict(torch.load('MLM_wikitext.pt'))"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHKUss5DTMif"
      },
      "source": [
        "# Train Text GRU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDLawItUVFIu"
      },
      "source": [
        "class TextGRU(nn.Module):\n",
        "    '''\n",
        "    Recurrent Neural Network based on Gated Rectified Unit for text classification [1].\n",
        "    Credits for implementation: [2]\n",
        "    Inputs:\n",
        "        vocab_size - size of vocabulary of dataset used\n",
        "        emb_dim - embedding space dimension\n",
        "        hidden_dim - dimension of hidden state of RNN\n",
        "        output_dim - dimension of the output (number of classes in our case)\n",
        "        dropout - drop out probability\n",
        "        pad_idx - padding index for sequences\n",
        "    [1]: Empirical evaluation of gated recurrent neural networks on sequence modeling, 2014.\n",
        "    [2]: https://github.com/etomoscow/DL-in-NLP/blob/master/hw3/task3_sentiment_rnn.ipynb\n",
        "    '''\n",
        "\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, out_dim, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, num_layers=1, bidirectional=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        packed_output, hidden = self.rnn(packed_embedded)\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "        return self.fc(hidden)\n",
        "\n",
        "    def forward_on_embeddings(self, embed, text_lengths):\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embed, text_lengths)\n",
        "        packed_output, hidden = self.rnn(packed_embedded)\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "        return self.fc(hidden) "
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p35fBqInTMQt",
        "outputId": "a0ad0bdf-498d-4f1f-fe92-7240612d499f"
      },
      "source": [
        "#load the classifier\n",
        "\n",
        "classifier = TextGRU(vocab_size=len(TEXT.vocab), emb_dim=100, hidden_dim=128, out_dim=6, dropout=0.1, pad_idx=TEXT.vocab.stoi[TEXT.pad_token])\n",
        "# classifier.load_state_dict(torch.load('textrnn_trec.pt'))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMxUqUoBctpP"
      },
      "source": [
        "criterion_ = nn.CrossEntropyLoss()\n",
        "criterion_ = criterion_.to(device)\n",
        "classifier = classifier.to(device)\n",
        "optimizer_ = torch.optim.Adam(classifier.parameters(), lr=0.001)"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2e-DKorct-f"
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim=1, keepdim=True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    correct = correct.detach().to('cpu')\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu0lC-KWd0Cm"
      },
      "source": [
        "def get_text_length(batch):\n",
        "    global device\n",
        "    global TEXT\n",
        "    result = []\n",
        "    for i in range(batch.shape[1]):\n",
        "        result.append((sum(batch[:, i] != TEXT.vocab.stoi[TEXT.pad_token]).item()))\n",
        "    return torch.tensor(result, dtype=int, device='cpu')"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPsdfGnocuRx"
      },
      "source": [
        "def train_gru(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text = batch.text\n",
        "        text_lengths = get_text_length(text)\n",
        "        predictions = model(text, text_lengths)\n",
        "        loss = criterion(predictions, batch.label.long())\n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWw6VPuAcuh6"
      },
      "source": [
        "def eval_gru(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text = batch.text\n",
        "            text_lengths = get_text_length(text)\n",
        "            predictions = model(text, text_lengths)\n",
        "            loss = criterion(predictions, batch.label.long())\n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17CceHgvid4h"
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTJ9o0mkcux5",
        "outputId": "c118be7f-dc0c-40cd-c99f-33d3de4358d9"
      },
      "source": [
        "N_EPOCHS = 50\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_gru(classifier, train_iterator, optimizer_, criterion_)\n",
        "    valid_loss, valid_acc = eval_gru(classifier, valid_iterator, criterion_)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'textrnn_trec.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 1.316 | Train Acc: 48.76%\n",
            "\t Val. Loss: 1.026 |  Val. Acc: 59.77%\n",
            "Epoch: 02 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.862 | Train Acc: 67.43%\n",
            "\t Val. Loss: 0.792 |  Val. Acc: 69.98%\n",
            "Epoch: 03 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.647 | Train Acc: 76.08%\n",
            "\t Val. Loss: 0.745 |  Val. Acc: 72.43%\n",
            "Epoch: 04 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.484 | Train Acc: 82.67%\n",
            "\t Val. Loss: 0.717 |  Val. Acc: 75.10%\n",
            "Epoch: 05 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.348 | Train Acc: 87.52%\n",
            "\t Val. Loss: 0.757 |  Val. Acc: 75.01%\n",
            "Epoch: 06 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.244 | Train Acc: 92.37%\n",
            "\t Val. Loss: 0.773 |  Val. Acc: 77.19%\n",
            "Epoch: 07 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.158 | Train Acc: 95.21%\n",
            "\t Val. Loss: 0.844 |  Val. Acc: 75.76%\n",
            "Epoch: 08 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.100 | Train Acc: 97.21%\n",
            "\t Val. Loss: 0.901 |  Val. Acc: 76.60%\n",
            "Epoch: 09 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.066 | Train Acc: 98.29%\n",
            "\t Val. Loss: 0.911 |  Val. Acc: 77.01%\n",
            "Epoch: 10 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.042 | Train Acc: 98.91%\n",
            "\t Val. Loss: 0.915 |  Val. Acc: 77.44%\n",
            "Epoch: 11 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.031 | Train Acc: 99.22%\n",
            "\t Val. Loss: 0.981 |  Val. Acc: 77.42%\n",
            "Epoch: 12 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.026 | Train Acc: 99.40%\n",
            "\t Val. Loss: 0.960 |  Val. Acc: 78.61%\n",
            "Epoch: 13 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.018 | Train Acc: 99.58%\n",
            "\t Val. Loss: 0.972 |  Val. Acc: 79.15%\n",
            "Epoch: 14 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.013 | Train Acc: 99.79%\n",
            "\t Val. Loss: 0.986 |  Val. Acc: 78.55%\n",
            "Epoch: 15 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.010 | Train Acc: 99.82%\n",
            "\t Val. Loss: 1.038 |  Val. Acc: 77.82%\n",
            "Epoch: 16 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.009 | Train Acc: 99.90%\n",
            "\t Val. Loss: 0.990 |  Val. Acc: 78.68%\n",
            "Epoch: 17 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.006 | Train Acc: 99.90%\n",
            "\t Val. Loss: 1.068 |  Val. Acc: 77.96%\n",
            "Epoch: 18 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.006 | Train Acc: 99.90%\n",
            "\t Val. Loss: 1.031 |  Val. Acc: 78.97%\n",
            "Epoch: 19 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.007 | Train Acc: 99.90%\n",
            "\t Val. Loss: 1.057 |  Val. Acc: 79.09%\n",
            "Epoch: 20 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.015 | Train Acc: 99.51%\n",
            "\t Val. Loss: 1.156 |  Val. Acc: 76.72%\n",
            "Epoch: 21 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.009 | Train Acc: 99.84%\n",
            "\t Val. Loss: 1.032 |  Val. Acc: 78.87%\n",
            "Epoch: 22 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.009 | Train Acc: 99.77%\n",
            "\t Val. Loss: 1.129 |  Val. Acc: 79.28%\n",
            "Epoch: 23 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.004 | Train Acc: 99.95%\n",
            "\t Val. Loss: 1.181 |  Val. Acc: 78.85%\n",
            "Epoch: 24 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.005 | Train Acc: 99.97%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 77.67%\n",
            "Epoch: 25 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.005 | Train Acc: 99.90%\n",
            "\t Val. Loss: 1.233 |  Val. Acc: 78.39%\n",
            "Epoch: 26 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.97%\n",
            "\t Val. Loss: 1.217 |  Val. Acc: 79.70%\n",
            "Epoch: 27 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.95%\n",
            "\t Val. Loss: 1.156 |  Val. Acc: 79.04%\n",
            "Epoch: 28 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.92%\n",
            "\t Val. Loss: 1.335 |  Val. Acc: 77.48%\n",
            "Epoch: 29 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.007 | Train Acc: 99.79%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 78.91%\n",
            "Epoch: 30 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.004 | Train Acc: 99.92%\n",
            "\t Val. Loss: 1.262 |  Val. Acc: 78.56%\n",
            "Epoch: 31 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.005 | Train Acc: 99.87%\n",
            "\t Val. Loss: 1.276 |  Val. Acc: 79.49%\n",
            "Epoch: 32 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.004 | Train Acc: 99.92%\n",
            "\t Val. Loss: 1.223 |  Val. Acc: 78.98%\n",
            "Epoch: 33 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.008 | Train Acc: 99.79%\n",
            "\t Val. Loss: 1.153 |  Val. Acc: 79.09%\n",
            "Epoch: 34 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.005 | Train Acc: 99.87%\n",
            "\t Val. Loss: 1.236 |  Val. Acc: 78.65%\n",
            "Epoch: 35 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.009 | Train Acc: 99.74%\n",
            "\t Val. Loss: 1.205 |  Val. Acc: 78.61%\n",
            "Epoch: 36 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.006 | Train Acc: 99.84%\n",
            "\t Val. Loss: 1.196 |  Val. Acc: 78.88%\n",
            "Epoch: 37 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.92%\n",
            "\t Val. Loss: 1.326 |  Val. Acc: 78.27%\n",
            "Epoch: 38 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.004 | Train Acc: 99.87%\n",
            "\t Val. Loss: 1.379 |  Val. Acc: 77.73%\n",
            "Epoch: 39 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.004 | Train Acc: 99.82%\n",
            "\t Val. Loss: 1.394 |  Val. Acc: 77.78%\n",
            "Epoch: 40 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.004 | Train Acc: 99.92%\n",
            "\t Val. Loss: 1.311 |  Val. Acc: 78.14%\n",
            "Epoch: 41 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.004 | Train Acc: 99.87%\n",
            "\t Val. Loss: 1.366 |  Val. Acc: 78.33%\n",
            "Epoch: 42 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 78.93%\n",
            "Epoch: 43 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
            "\t Val. Loss: 1.349 |  Val. Acc: 79.09%\n",
            "Epoch: 44 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.001 | Train Acc: 99.97%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 79.22%\n",
            "Epoch: 45 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.001 | Train Acc: 99.97%\n",
            "\t Val. Loss: 1.326 |  Val. Acc: 79.04%\n",
            "Epoch: 46 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
            "\t Val. Loss: 1.311 |  Val. Acc: 79.03%\n",
            "Epoch: 47 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.005 | Train Acc: 99.87%\n",
            "\t Val. Loss: 1.317 |  Val. Acc: 78.45%\n",
            "Epoch: 48 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.004 | Train Acc: 99.90%\n",
            "\t Val. Loss: 1.397 |  Val. Acc: 77.37%\n",
            "Epoch: 49 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.92%\n",
            "\t Val. Loss: 1.273 |  Val. Acc: 78.81%\n",
            "Epoch: 50 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.005 | Train Acc: 99.87%\n",
            "\t Val. Loss: 1.219 |  Val. Acc: 79.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPQ119SrjkIF"
      },
      "source": [
        "# Prepare baseline attack - SamplingFool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRm1uNa9KKHe"
      },
      "source": [
        "SamplingFool "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV-UAK93NVnb"
      },
      "source": [
        "`output` is a 3D-tensor of shape `[sequnce_len, batch_size, vocab_size]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjLyF1l2jM_m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67d778d2-3575-4c06-c2c5-bd49669a141a"
      },
      "source": [
        "model.eval() # Turn on the evaluation mode\n",
        "classifier.eval() #turn on evaluation mode for classifier\n",
        "with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "        fullbatch = batch\n",
        "        data = batch.text\n",
        "        src_mask = model.generate_square_subsequent_mask(data.size(0))\n",
        "        src_mask = src_mask.to(device)\n",
        "        output = model(data, src_mask)\n",
        "        for tau in torch.arange(1e-2, 1e5, 20):\n",
        "            gumbel_output = torch.nn.functional.gumbel_softmax(output, tau=tau, hard=True, eps=1e-10, dim=2)\n",
        "            try:\n",
        "                attack_output = classifier(torch.argmax(gumbel_output, dim=2), get_text_length(torch.argmax(gumbel_output, dim=2)))\n",
        "            except RuntimeError:\n",
        "                continue\n",
        "            original_output = classifier(data, get_text_length(data))\n",
        "            if categorical_accuracy(attack_output, fullbatch.label).item() + 0.1 < categorical_accuracy(original_output, fullbatch.label).item():\n",
        "                print('Attack was successfull, accuracy drop: {:.3f}%, tau value:{}'.format((categorical_accuracy(attack_output, fullbatch.label).item()-categorical_accuracy(original_output, fullbatch.label).item())*100, tau))"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attack was successfull, accuracy drop: -10.938%, tau value:4360.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:4460.009765625\n",
            "Attack was successfull, accuracy drop: -12.500%, tau value:22540.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:35780.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:40080.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:49900.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:51400.01171875\n",
            "Attack was successfull, accuracy drop: -12.500%, tau value:57320.01171875\n",
            "Attack was successfull, accuracy drop: -12.500%, tau value:62620.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:68120.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:70640.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:74780.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:88680.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:89260.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:90480.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:91220.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:98520.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:99100.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:19680.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:77380.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:2580.010009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:6180.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:7460.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:15420.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:16560.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:17320.009765625\n",
            "Attack was successfull, accuracy drop: -12.500%, tau value:18000.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:18780.009765625\n",
            "Attack was successfull, accuracy drop: -12.500%, tau value:20320.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:21140.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:23740.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:27260.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:30600.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:31680.009765625\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:40640.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:40760.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:45060.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:45560.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:49160.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:51580.01171875\n",
            "Attack was successfull, accuracy drop: -12.500%, tau value:51600.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:55200.01171875\n",
            "Attack was successfull, accuracy drop: -12.500%, tau value:57920.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:61000.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:61480.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:62300.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:63980.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:64560.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:64900.01171875\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:68400.0078125\n",
            "Attack was successfull, accuracy drop: -12.500%, tau value:72820.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:72880.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:75900.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:78340.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:78680.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:79500.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:80280.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:80700.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:81780.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:89120.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:91880.0078125\n",
            "Attack was successfull, accuracy drop: -10.938%, tau value:94300.0078125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-177-97a6f085d303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0moriginal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_text_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcategorical_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattack_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcategorical_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attack was successfull, accuracy drop: {:.3f}%, tau value:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattack_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcategorical_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-163-9841510e0c1a>\u001b[0m in \u001b[0;36mget_text_length\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCYDBpQDycc2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}